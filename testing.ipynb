{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"173ofYxlBU6H7xa8gghzcKONPxm_3jUHt","authorship_tag":"ABX9TyPnxRN45euu7vXwAfVxOuXB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SwCvckossDr","executionInfo":{"status":"ok","timestamp":1701131184674,"user_tz":480,"elapsed":14445,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"08451fd8-ac38-4251-81cf-3054e0f091db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.37.1)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"]}],"source":["!pip install torch numpy transformers datasets tiktoken wandb tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaH__RGYs_xi","executionInfo":{"status":"ok","timestamp":1701131185498,"user_tz":480,"elapsed":826,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"2c8880e9-5d44-4e23-f12b-66e35aa7ae33"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Fall/CS229S/cs229s-final-project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47xHdkM2s_nX","executionInfo":{"status":"ok","timestamp":1701131185689,"user_tz":480,"elapsed":194,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"a0a68317-0f2a-41fd-8d2e-280613ae2289"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Fall/CS229S/cs229s-final-project\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","from model import GPTConfig, GPT"],"metadata":{"id":"JL1KXKlOtCMx","executionInfo":{"status":"ok","timestamp":1701131189393,"user_tz":480,"elapsed":3705,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","torch.cuda.empty_cache()\n","before_inference = torch.cuda.memory_allocated()\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'out' # ignored if init_from is not 'resume'\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","elif init_from.startswith('gpt2'):\n","    # init from a given GPT-2 model\n","    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n","\n","model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n","\n","# look for the meta pickle in case it is available in the dataset folder\n","load_meta = False\n","if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n","    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n","    load_meta = os.path.exists(meta_path)\n","if load_meta:\n","    print(f\"Loading meta from {meta_path}...\")\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    # TODO want to make this more general to arbitrary encoder/decoder schemes\n","    stoi, itos = meta['stoi'], meta['itos']\n","    encode = lambda s: [stoi[c] for c in s]\n","    decode = lambda l: ''.join([itos[i] for i in l])\n","else:\n","    # ok let's assume gpt-2 encodings by default\n","    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n","    enc = tiktoken.get_encoding(\"gpt2\")\n","    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","    decode = lambda l: enc.decode(l)\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QspuGVs-5dfq","executionInfo":{"status":"ok","timestamp":1701130973732,"user_tz":480,"elapsed":17114,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"18ae0560-2bb7-43d0-ea2b-4d4a68ed47f8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 123.65M\n","No meta.pkl found, assuming GPT-2 encodings...\n"]}]},{"cell_type":"code","source":["model.quantize = False\n","\n","start_time = time.time()\n","y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","print(decode(y[0].tolist()))\n","print('---------------')\n","after_inference = torch.cuda.memory_allocated()\n","\n","print(f\"Took {time.time() - start_time} seconds\")\n","print(f\"Memory used for inference: {after_inference - before_inference} bytes\")\n","print(before_inference)\n","print(after_inference)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"if0pcZ5DNCwL","executionInfo":{"status":"ok","timestamp":1701130983021,"user_tz":480,"elapsed":9302,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"89c100d6-61f1-4245-c606-8672b43ddab4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","<|endoftext|><|endoftext|><|endoftext|> All four films were created in the same year . \n","<|endoftext|><|endoftext|> The script was written by Thomas Widdicott ( who also wrote for La Jetée ) and Jean @-@ Louis Gaudé ( the voices of Arle ) . It was written by Reine Kunz and Jean @-@ Luc Picard . It was a \" black @-@ and @-@ white \" story which was divided into two parts , with the story in the right order . Many of the character designs were drawn by Brian Kelly , who also drew the character designs for the animated episodes . The story was also written by the series ' art director , Kevin Macdonald . \n","<|endoftext|> A short time before the beginning of the series , \" The Three Doctors \" takes place on a bridge in the Vale of Glamorgan , in a city called The Vale . In \" The Three Doctors \" , the Doctor is on the bridge . With the Doctor 's health deteriorating , the Doctor 's friend Steven befriends him , and the Doctor and Gabriel <unk> discover that the Doctor was once a member of The Doctors . They plan to investigate the location of the Doctor 's lab in order to find a cure for their condition . The Doctor , who is in his thirties , is sedated by an alien woman named Alastor . In order to perform an experiment on Alastor , the Doctor and Gabriel are both sealed in a small chamber beneath the volcano . The Doctor , Gabriel and the others make their way to the cave , which contains the cave 's exit . \n","<|endoftext|> = = = Rescuing = = = \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = = Reception = = = \n","<|endoftext|> = = = = Cold blooded = = = = \n","<|endoftext|><|endoftext|><|endoftext|> = = = Invertebrates = = = \n","<|endoftext|><|endoftext|> In order to release the film , a total of 600 @,@ 000 people were consulted on the film 's shooting schedule in September 2012 . Media @-@ savvy audiences were then invited to participate in an online contest , in which one player was allowed to enter the public image and upload a photo of himself and the other player was allowed to write a personal message to be shown on the screen . \n","<|endoftext|><|endoftext|> The exact composition of the album was not known in 1993 when producer Jim Irsay and the band 's musical director , Tom Roper , began organizing a few days prior\n","---------------\n","Took 9.182231903076172 seconds\n","Memory used for inference: 2003360768 bytes\n","0\n","2003360768\n"]}]},{"cell_type":"code","source":["import tiktoken\n","\n","torch.cuda.empty_cache()\n","before_inference = torch.cuda.memory_allocated()\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'out' # ignored if init_from is not 'resume'\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","elif init_from.startswith('gpt2'):\n","    # init from a given GPT-2 model\n","    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n","\n","model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n","\n","# look for the meta pickle in case it is available in the dataset folder\n","load_meta = False\n","if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n","    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n","    load_meta = os.path.exists(meta_path)\n","if load_meta:\n","    print(f\"Loading meta from {meta_path}...\")\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    # TODO want to make this more general to arbitrary encoder/decoder schemes\n","    stoi, itos = meta['stoi'], meta['itos']\n","    encode = lambda s: [stoi[c] for c in s]\n","    decode = lambda l: ''.join([itos[i] for i in l])\n","else:\n","    # ok let's assume gpt-2 encodings by default\n","    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n","    enc = tiktoken.get_encoding(\"gpt2\")\n","    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","    decode = lambda l: enc.decode(l)\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25gjsB50RA9K","executionInfo":{"status":"ok","timestamp":1701131206911,"user_tz":480,"elapsed":17519,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"e55939d1-444c-46d3-a985-abc9abc737f0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 123.65M\n","No meta.pkl found, assuming GPT-2 encodings...\n","1994836992\n"]}]},{"cell_type":"code","source":["model.quantize = True\n","\n","start_time = time.time()\n","model.quantize_transformers()\n","y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","print(decode(y[0].tolist()))\n","print('---------------')\n","after_inference = torch.cuda.memory_allocated()\n","\n","\n","print(f\"Took {time.time() - start_time} seconds\")\n","print(f\"Memory used for inference with quantization: {after_inference - before_inference} bytes\")\n","print(before_inference)\n","print(after_inference)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-LO_NfGDFBi","executionInfo":{"status":"ok","timestamp":1701130904409,"user_tz":480,"elapsed":21730,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"a391e782-2329-4d57-bc31-3971143e57bc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","- * * AllMusic \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = = = 2008 : Total solar power generation , EIA Appendix 1 = = = = \n","<|endoftext|> The site had an area of low vegetation , and was rich in pine and other species . A large boulder was discovered in an early excavation , and it was believed to have been the source of a \" black hole \" . In the early 1900s a black hole was seen , but it has since been removed . It is thought to have been the source of the Black Tower explosion . \n","<|endoftext|> Despite knowing the war was imminent , in the first days of the American Revolution the first United States militia was established in the state . In March , 1803 , it was organized , with the military name \" Fort Jefferson \" due to its mission of providing militia to fight in the Revolutionary Wars . The fort was named Fort Jefferson , which was intended to house a force of militia ; but the fort 's name was actually a reference to the former Washington , D.C. Fort , which was built by James Jay , and is the present @-@ day capital of the United States . \n","<|endoftext|><|endoftext|> The Nests ( n ) was the number of birds in a nest during a time of estination or in development . It was the weight of the nest that was moved around , and it was the nest 's average weight that the bird was likely to have . If the nest weighed up to 2 @.@ 9 or 2 @.@ 0 pounds ( 1 @.@ 0 kg ) , the birds could be dangerous . If the nest weighed 1 @.@ 0 pounds ( 0 @.@ 80 kg ) , then the birds could be dangerous . \n","<|endoftext|> = = = Genus <unk> = = = \n","<|endoftext|><|endoftext|><|endoftext|> = = = = Filming , recording and editing , and music = = = \n","<|endoftext|> The first major event was the Grand Prix of Formula One at the Circuit of Monte Carlo in Switzerland . Formula One drivers competed at two different points , namely the Grand Prix of Monaco ( an event that is not officially known as a Formula One event ) and the Grand Prix of Spa @-@ Santo da Luz . The Grand Prix of Monaco was held on 5 August 1976 to an estimated audience of 150 @,@ 000 . The race took place in a variety of venues and venues in Argentina , Belgium , Finland , Denmark , Germany , Spain and Switzerland . The Grand\n","---------------\n","Took 21.62495732307434 seconds\n","Memory used for inference with quantization: 1752224768 bytes\n","0\n","1752224768\n"]}]}]}