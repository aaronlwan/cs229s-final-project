{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["yw4hozax6Kmr"],"mount_file_id":"173ofYxlBU6H7xa8gghzcKONPxm_3jUHt","authorship_tag":"ABX9TyNpdPWS57a1xJ83SJVlITIf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Imports and Google Drive Auth"],"metadata":{"id":"yw4hozax6Kmr"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SwCvckossDr","executionInfo":{"status":"ok","timestamp":1701820978047,"user_tz":480,"elapsed":24551,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"63a0e473-cc6e-4218-8f50-aec1b75fc269"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, pyarrow-hotfix, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 wandb-0.16.1\n"]}],"source":["!pip install torch numpy transformers datasets tiktoken wandb tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaH__RGYs_xi","executionInfo":{"status":"ok","timestamp":1701821058525,"user_tz":480,"elapsed":80482,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"5291b54e-26d9-4fde-ec3a-e422487dab60"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Fall/CS229S/cs229s-final-project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47xHdkM2s_nX","executionInfo":{"status":"ok","timestamp":1701821058904,"user_tz":480,"elapsed":381,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"b270d1cb-8d93-43e6-a52b-1f193a0fb221"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Fall/CS229S/cs229s-final-project\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","from model import GPTConfig, GPT\n","\n","import tiktoken"],"metadata":{"id":"JL1KXKlOtCMx","executionInfo":{"status":"ok","timestamp":1701821065398,"user_tz":480,"elapsed":6495,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Debugging + Save Quantization Model"],"metadata":{"id":"qLO0Pdo8AU2J"}},{"cell_type":"code","source":["import tiktoken\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'wikitext' # ignored if init_from is not 'resume'\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    print(f'Loading from {ckpt_path}')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","elif init_from.startswith('gpt2'):\n","    # init from a given GPT-2 model\n","    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n","\n","model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n","\n","# look for the meta pickle in case it is available in the dataset folder\n","load_meta = False\n","if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n","    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n","    load_meta = os.path.exists(meta_path)\n","if load_meta:\n","    print(f\"Loading meta from {meta_path}...\")\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    # TODO want to make this more general to arbitrary encoder/decoder schemes\n","    stoi, itos = meta['stoi'], meta['itos']\n","    encode = lambda s: [stoi[c] for c in s]\n","    decode = lambda l: ''.join([itos[i] for i in l])\n","else:\n","    # ok let's assume gpt-2 encodings by default\n","    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n","    enc = tiktoken.get_encoding(\"gpt2\")\n","    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","    decode = lambda l: enc.decode(l)\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEWFaMmDCJeP","executionInfo":{"status":"ok","timestamp":1701821092996,"user_tz":480,"elapsed":27599,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"92f2409a-84dc-44d5-f905-160153066eb8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading from wikitext/ckpt.pt\n","Model in quantized mode\n","number of parameters: 123.65M\n","No meta.pkl found, assuming GPT-2 encodings...\n"]}]},{"cell_type":"code","source":["total_bytes = 0\n","for param in model.parameters():\n","    total_bytes += param.numel() * param.element_size()\n","\n","print(f\"Total memory usage in bytes: {total_bytes}\")\n","print(f\"Total memory usage in megabytes: {total_bytes / (1024 ** 2)}\")\n","print(param.element_size())\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name}, Type: {param.dtype}, Size: {param.data.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-UCtrHUAZv_","executionInfo":{"status":"ok","timestamp":1701821092997,"user_tz":480,"elapsed":9,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"535c84b6-4e12-4146-84f1-4d90da51770c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Total memory usage in bytes: 497759232\n","Total memory usage in megabytes: 474.7001953125\n","4\n","Layer: transformer.wte.weight, Type: torch.float32, Size: torch.Size([50257, 768])\n","Layer: transformer.wpe.weight, Type: torch.float32, Size: torch.Size([1024, 768])\n","Layer: transformer.h.0.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.0.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.0.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.0.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.0.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.0.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.0.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.0.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.0.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.1.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.1.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.1.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.1.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.1.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.1.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.1.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.2.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.2.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.2.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.2.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.2.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.2.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.2.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.3.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.3.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.3.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.3.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.3.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.3.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.3.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.4.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.4.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.4.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.4.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.4.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.4.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.4.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.5.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.5.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.5.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.5.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.5.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.5.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.5.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.6.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.6.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.6.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.6.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.6.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.6.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.6.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.7.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.7.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.7.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.7.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.7.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.7.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.7.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.8.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.8.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.8.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.8.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.8.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.8.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.8.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.9.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.9.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.9.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.9.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.9.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.9.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.9.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.10.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.10.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.10.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.10.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.10.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.10.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.10.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_1.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_1.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.attn.c_attn.weight, Type: torch.float32, Size: torch.Size([2304, 768])\n","Layer: transformer.h.11.attn.c_attn.bias, Type: torch.float32, Size: torch.Size([2304])\n","Layer: transformer.h.11.attn.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 768])\n","Layer: transformer.h.11.attn.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_2.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_2.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.h.11.mlp.c_fc.weight, Type: torch.float32, Size: torch.Size([3072, 768])\n","Layer: transformer.h.11.mlp.c_fc.bias, Type: torch.float32, Size: torch.Size([3072])\n","Layer: transformer.h.11.mlp.c_proj.weight, Type: torch.float32, Size: torch.Size([768, 3072])\n","Layer: transformer.h.11.mlp.c_proj.bias, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.ln_f.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.ln_f.bias, Type: torch.float32, Size: torch.Size([768])\n"]}]},{"cell_type":"code","source":["model.quantize_transformers()\n","print(model.quantization_scales)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEuSxhFI9QFC","executionInfo":{"status":"ok","timestamp":1701821093186,"user_tz":480,"elapsed":197,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"786d2d1a-44c5-4e6e-cfea-0c63ef353b0d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'transformer.h.0.ln_1.weight': tensor(501.8847, device='cuda:0'), 'transformer.h.0.ln_1.bias': tensor(490.1018, device='cuda:0'), 'transformer.h.0.attn.c_attn.weight': tensor(44.7227, device='cuda:0'), 'transformer.h.0.attn.c_attn.bias': tensor(94.9118, device='cuda:0'), 'transformer.h.0.attn.c_proj.weight': tensor(38.4227, device='cuda:0'), 'transformer.h.0.attn.c_proj.bias': tensor(47.3122, device='cuda:0'), 'transformer.h.0.ln_2.weight': tensor(83.9078, device='cuda:0'), 'transformer.h.0.ln_2.bias': tensor(171.3022, device='cuda:0'), 'transformer.h.0.mlp.c_fc.weight': tensor(27.8004, device='cuda:0'), 'transformer.h.0.mlp.c_fc.bias': tensor(170.8822, device='cuda:0'), 'transformer.h.0.mlp.c_proj.weight': tensor(20.7393, device='cuda:0'), 'transformer.h.0.mlp.c_proj.bias': tensor(85.9377, device='cuda:0'), 'transformer.h.1.ln_1.weight': tensor(192.7835, device='cuda:0'), 'transformer.h.1.ln_1.bias': tensor(190.9859, device='cuda:0'), 'transformer.h.1.attn.c_attn.weight': tensor(103.1711, device='cuda:0'), 'transformer.h.1.attn.c_attn.bias': tensor(65.4315, device='cuda:0'), 'transformer.h.1.attn.c_proj.weight': tensor(27.0571, device='cuda:0'), 'transformer.h.1.attn.c_proj.bias': tensor(101.8587, device='cuda:0'), 'transformer.h.1.ln_2.weight': tensor(281.8196, device='cuda:0'), 'transformer.h.1.ln_2.bias': tensor(216.0666, device='cuda:0'), 'transformer.h.1.mlp.c_fc.weight': tensor(55.6911, device='cuda:0'), 'transformer.h.1.mlp.c_fc.bias': tensor(193.1642, device='cuda:0'), 'transformer.h.1.mlp.c_proj.weight': tensor(9.2751, device='cuda:0'), 'transformer.h.1.mlp.c_proj.bias': tensor(79.6125, device='cuda:0'), 'transformer.h.2.ln_1.weight': tensor(134.1642, device='cuda:0'), 'transformer.h.2.ln_1.bias': tensor(116.0916, device='cuda:0'), 'transformer.h.2.attn.c_attn.weight': tensor(76.0895, device='cuda:0'), 'transformer.h.2.attn.c_attn.bias': tensor(86.8576, device='cuda:0'), 'transformer.h.2.attn.c_proj.weight': tensor(55.3764, device='cuda:0'), 'transformer.h.2.attn.c_proj.bias': tensor(246.7206, device='cuda:0'), 'transformer.h.2.ln_2.weight': tensor(177.0001, device='cuda:0'), 'transformer.h.2.ln_2.bias': tensor(196.4717, device='cuda:0'), 'transformer.h.2.mlp.c_fc.weight': tensor(12.0679, device='cuda:0'), 'transformer.h.2.mlp.c_fc.bias': tensor(73.9293, device='cuda:0'), 'transformer.h.2.mlp.c_proj.weight': tensor(8.4678, device='cuda:0'), 'transformer.h.2.mlp.c_proj.bias': tensor(81.1532, device='cuda:0'), 'transformer.h.3.ln_1.weight': tensor(165.6309, device='cuda:0'), 'transformer.h.3.ln_1.bias': tensor(73.0905, device='cuda:0'), 'transformer.h.3.attn.c_attn.weight': tensor(67.2003, device='cuda:0'), 'transformer.h.3.attn.c_attn.bias': tensor(178.3555, device='cuda:0'), 'transformer.h.3.attn.c_proj.weight': tensor(60.8180, device='cuda:0'), 'transformer.h.3.attn.c_proj.bias': tensor(123.5897, device='cuda:0'), 'transformer.h.3.ln_2.weight': tensor(110.1740, device='cuda:0'), 'transformer.h.3.ln_2.bias': tensor(289.7144, device='cuda:0'), 'transformer.h.3.mlp.c_fc.weight': tensor(48.0373, device='cuda:0'), 'transformer.h.3.mlp.c_fc.bias': tensor(102.7957, device='cuda:0'), 'transformer.h.3.mlp.c_proj.weight': tensor(7.4539, device='cuda:0'), 'transformer.h.3.mlp.c_proj.bias': tensor(68.2286, device='cuda:0'), 'transformer.h.4.ln_1.weight': tensor(189.5896, device='cuda:0'), 'transformer.h.4.ln_1.bias': tensor(81.8323, device='cuda:0'), 'transformer.h.4.attn.c_attn.weight': tensor(38.2450, device='cuda:0'), 'transformer.h.4.attn.c_attn.bias': tensor(46.2161, device='cuda:0'), 'transformer.h.4.attn.c_proj.weight': tensor(69.1690, device='cuda:0'), 'transformer.h.4.attn.c_proj.bias': tensor(197.9954, device='cuda:0'), 'transformer.h.4.ln_2.weight': tensor(113.1744, device='cuda:0'), 'transformer.h.4.ln_2.bias': tensor(896.3248, device='cuda:0'), 'transformer.h.4.mlp.c_fc.weight': tensor(58.9788, device='cuda:0'), 'transformer.h.4.mlp.c_fc.bias': tensor(171.1866, device='cuda:0'), 'transformer.h.4.mlp.c_proj.weight': tensor(26.6624, device='cuda:0'), 'transformer.h.4.mlp.c_proj.bias': tensor(81.1859, device='cuda:0'), 'transformer.h.5.ln_1.weight': tensor(164.1738, device='cuda:0'), 'transformer.h.5.ln_1.bias': tensor(117.6082, device='cuda:0'), 'transformer.h.5.attn.c_attn.weight': tensor(91.5912, device='cuda:0'), 'transformer.h.5.attn.c_attn.bias': tensor(226.9202, device='cuda:0'), 'transformer.h.5.attn.c_proj.weight': tensor(64.3864, device='cuda:0'), 'transformer.h.5.attn.c_proj.bias': tensor(181.5699, device='cuda:0'), 'transformer.h.5.ln_2.weight': tensor(90.1438, device='cuda:0'), 'transformer.h.5.ln_2.bias': tensor(399.7993, device='cuda:0'), 'transformer.h.5.mlp.c_fc.weight': tensor(64.8294, device='cuda:0'), 'transformer.h.5.mlp.c_fc.bias': tensor(189.1982, device='cuda:0'), 'transformer.h.5.mlp.c_proj.weight': tensor(44.9104, device='cuda:0'), 'transformer.h.5.mlp.c_proj.bias': tensor(101.6104, device='cuda:0'), 'transformer.h.6.ln_1.weight': tensor(161.1443, device='cuda:0'), 'transformer.h.6.ln_1.bias': tensor(83.4212, device='cuda:0'), 'transformer.h.6.attn.c_attn.weight': tensor(78.3503, device='cuda:0'), 'transformer.h.6.attn.c_attn.bias': tensor(155.7361, device='cuda:0'), 'transformer.h.6.attn.c_proj.weight': tensor(68.3638, device='cuda:0'), 'transformer.h.6.attn.c_proj.bias': tensor(322.2487, device='cuda:0'), 'transformer.h.6.ln_2.weight': tensor(95.3251, device='cuda:0'), 'transformer.h.6.ln_2.bias': tensor(281.3669, device='cuda:0'), 'transformer.h.6.mlp.c_fc.weight': tensor(59.8931, device='cuda:0'), 'transformer.h.6.mlp.c_fc.bias': tensor(185.7360, device='cuda:0'), 'transformer.h.6.mlp.c_proj.weight': tensor(45.9970, device='cuda:0'), 'transformer.h.6.mlp.c_proj.bias': tensor(121.7020, device='cuda:0'), 'transformer.h.7.ln_1.weight': tensor(154.3819, device='cuda:0'), 'transformer.h.7.ln_1.bias': tensor(106.1733, device='cuda:0'), 'transformer.h.7.attn.c_attn.weight': tensor(70.6231, device='cuda:0'), 'transformer.h.7.attn.c_attn.bias': tensor(167.4841, device='cuda:0'), 'transformer.h.7.attn.c_proj.weight': tensor(57.2942, device='cuda:0'), 'transformer.h.7.attn.c_proj.bias': tensor(246.3623, device='cuda:0'), 'transformer.h.7.ln_2.weight': tensor(98.7808, device='cuda:0'), 'transformer.h.7.ln_2.bias': tensor(202.4408, device='cuda:0'), 'transformer.h.7.mlp.c_fc.weight': tensor(103.5272, device='cuda:0'), 'transformer.h.7.mlp.c_fc.bias': tensor(152.4989, device='cuda:0'), 'transformer.h.7.mlp.c_proj.weight': tensor(28.2108, device='cuda:0'), 'transformer.h.7.mlp.c_proj.bias': tensor(108.4600, device='cuda:0'), 'transformer.h.8.ln_1.weight': tensor(136.0485, device='cuda:0'), 'transformer.h.8.ln_1.bias': tensor(87.3525, device='cuda:0'), 'transformer.h.8.attn.c_attn.weight': tensor(65.4546, device='cuda:0'), 'transformer.h.8.attn.c_attn.bias': tensor(145.6444, device='cuda:0'), 'transformer.h.8.attn.c_proj.weight': tensor(43.3395, device='cuda:0'), 'transformer.h.8.attn.c_proj.bias': tensor(109.7932, device='cuda:0'), 'transformer.h.8.ln_2.weight': tensor(119.2711, device='cuda:0'), 'transformer.h.8.ln_2.bias': tensor(182.5739, device='cuda:0'), 'transformer.h.8.mlp.c_fc.weight': tensor(87.3688, device='cuda:0'), 'transformer.h.8.mlp.c_fc.bias': tensor(127.8743, device='cuda:0'), 'transformer.h.8.mlp.c_proj.weight': tensor(23.7382, device='cuda:0'), 'transformer.h.8.mlp.c_proj.bias': tensor(104.5344, device='cuda:0'), 'transformer.h.9.ln_1.weight': tensor(132.8354, device='cuda:0'), 'transformer.h.9.ln_1.bias': tensor(100.4733, device='cuda:0'), 'transformer.h.9.attn.c_attn.weight': tensor(64.1534, device='cuda:0'), 'transformer.h.9.attn.c_attn.bias': tensor(121.6672, device='cuda:0'), 'transformer.h.9.attn.c_proj.weight': tensor(64.4294, device='cuda:0'), 'transformer.h.9.attn.c_proj.bias': tensor(67.0744, device='cuda:0'), 'transformer.h.9.ln_2.weight': tensor(135.1793, device='cuda:0'), 'transformer.h.9.ln_2.bias': tensor(226.4996, device='cuda:0'), 'transformer.h.9.mlp.c_fc.weight': tensor(45.8675, device='cuda:0'), 'transformer.h.9.mlp.c_fc.bias': tensor(208.7671, device='cuda:0'), 'transformer.h.9.mlp.c_proj.weight': tensor(23.2572, device='cuda:0'), 'transformer.h.9.mlp.c_proj.bias': tensor(85.7343, device='cuda:0'), 'transformer.h.10.ln_1.weight': tensor(136.1570, device='cuda:0'), 'transformer.h.10.ln_1.bias': tensor(116.5597, device='cuda:0'), 'transformer.h.10.attn.c_attn.weight': tensor(67.2551, device='cuda:0'), 'transformer.h.10.attn.c_attn.bias': tensor(138.6725, device='cuda:0'), 'transformer.h.10.attn.c_proj.weight': tensor(30.2380, device='cuda:0'), 'transformer.h.10.attn.c_proj.bias': tensor(33.0275, device='cuda:0'), 'transformer.h.10.ln_2.weight': tensor(116.1319, device='cuda:0'), 'transformer.h.10.ln_2.bias': tensor(184.0614, device='cuda:0'), 'transformer.h.10.mlp.c_fc.weight': tensor(49.7241, device='cuda:0'), 'transformer.h.10.mlp.c_fc.bias': tensor(119.3500, device='cuda:0'), 'transformer.h.10.mlp.c_proj.weight': tensor(11.5674, device='cuda:0'), 'transformer.h.10.mlp.c_proj.bias': tensor(94.4374, device='cuda:0'), 'transformer.h.11.ln_1.weight': tensor(132.9598, device='cuda:0'), 'transformer.h.11.ln_1.bias': tensor(126.3241, device='cuda:0'), 'transformer.h.11.attn.c_attn.weight': tensor(54.9319, device='cuda:0'), 'transformer.h.11.attn.c_attn.bias': tensor(156.6264, device='cuda:0'), 'transformer.h.11.attn.c_proj.weight': tensor(14.3364, device='cuda:0'), 'transformer.h.11.attn.c_proj.bias': tensor(23.6097, device='cuda:0'), 'transformer.h.11.ln_2.weight': tensor(102.2381, device='cuda:0'), 'transformer.h.11.ln_2.bias': tensor(304.4107, device='cuda:0'), 'transformer.h.11.mlp.c_fc.weight': tensor(65.3686, device='cuda:0'), 'transformer.h.11.mlp.c_fc.bias': tensor(103.3926, device='cuda:0'), 'transformer.h.11.mlp.c_proj.weight': tensor(13.8765, device='cuda:0'), 'transformer.h.11.mlp.c_proj.bias': tensor(294.2222, device='cuda:0')}\n"]}]},{"cell_type":"code","source":["# post quantization\n","\n","total_bytes = 0\n","for param in model.parameters():\n","    total_bytes += param.numel() * param.element_size()\n","\n","print(f\"Total memory usage in bytes: {total_bytes}\")\n","print(f\"Total memory usage in megabytes: {total_bytes / (1024 ** 2)}\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name}, Type: {param.dtype}, Size: {param.data.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4QNdNEgCS2U","executionInfo":{"status":"ok","timestamp":1701821787328,"user_tz":480,"elapsed":129,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"591ad495-01dc-42b6-8dfc-7c010c7e3592"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Total memory usage in bytes: 242595840\n","Total memory usage in megabytes: 231.357421875\n","4\n","Layer: transformer.wte.weight, Type: torch.float32, Size: torch.Size([50257, 768])\n","Layer: transformer.wpe.weight, Type: torch.float32, Size: torch.Size([1024, 768])\n","Layer: transformer.h.0.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.0.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.0.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.0.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.0.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.0.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.0.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.0.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.0.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.0.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.1.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.1.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.1.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.1.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.1.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.1.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.1.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.2.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.2.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.2.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.2.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.2.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.2.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.2.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.3.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.3.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.3.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.3.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.3.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.3.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.3.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.4.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.4.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.4.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.4.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.4.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.4.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.4.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.5.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.5.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.5.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.5.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.5.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.5.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.5.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.6.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.6.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.6.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.6.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.6.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.6.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.6.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.7.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.7.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.7.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.7.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.7.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.7.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.7.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.8.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.8.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.8.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.8.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.8.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.8.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.8.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.9.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.9.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.9.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.9.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.9.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.9.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.9.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.10.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.10.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.10.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.10.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.10.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.10.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.10.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_1.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_1.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.attn.c_attn.weight, Type: torch.int8, Size: torch.Size([2304, 768])\n","Layer: transformer.h.11.attn.c_attn.bias, Type: torch.int8, Size: torch.Size([2304])\n","Layer: transformer.h.11.attn.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 768])\n","Layer: transformer.h.11.attn.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_2.weight, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.ln_2.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.h.11.mlp.c_fc.weight, Type: torch.int8, Size: torch.Size([3072, 768])\n","Layer: transformer.h.11.mlp.c_fc.bias, Type: torch.int8, Size: torch.Size([3072])\n","Layer: transformer.h.11.mlp.c_proj.weight, Type: torch.int8, Size: torch.Size([768, 3072])\n","Layer: transformer.h.11.mlp.c_proj.bias, Type: torch.int8, Size: torch.Size([768])\n","Layer: transformer.ln_f.weight, Type: torch.float32, Size: torch.Size([768])\n","Layer: transformer.ln_f.bias, Type: torch.float32, Size: torch.Size([768])\n"]}]},{"cell_type":"code","source":["model.quantization_scales"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRe6rF4cSNUA","executionInfo":{"status":"ok","timestamp":1701821093397,"user_tz":480,"elapsed":212,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"cbfad844-3de6-49a6-a967-d3cafb25a89e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'transformer.h.0.ln_1.weight': tensor(501.8847, device='cuda:0'),\n"," 'transformer.h.0.ln_1.bias': tensor(490.1018, device='cuda:0'),\n"," 'transformer.h.0.attn.c_attn.weight': tensor(44.7227, device='cuda:0'),\n"," 'transformer.h.0.attn.c_attn.bias': tensor(94.9118, device='cuda:0'),\n"," 'transformer.h.0.attn.c_proj.weight': tensor(38.4227, device='cuda:0'),\n"," 'transformer.h.0.attn.c_proj.bias': tensor(47.3122, device='cuda:0'),\n"," 'transformer.h.0.ln_2.weight': tensor(83.9078, device='cuda:0'),\n"," 'transformer.h.0.ln_2.bias': tensor(171.3022, device='cuda:0'),\n"," 'transformer.h.0.mlp.c_fc.weight': tensor(27.8004, device='cuda:0'),\n"," 'transformer.h.0.mlp.c_fc.bias': tensor(170.8822, device='cuda:0'),\n"," 'transformer.h.0.mlp.c_proj.weight': tensor(20.7393, device='cuda:0'),\n"," 'transformer.h.0.mlp.c_proj.bias': tensor(85.9377, device='cuda:0'),\n"," 'transformer.h.1.ln_1.weight': tensor(192.7835, device='cuda:0'),\n"," 'transformer.h.1.ln_1.bias': tensor(190.9859, device='cuda:0'),\n"," 'transformer.h.1.attn.c_attn.weight': tensor(103.1711, device='cuda:0'),\n"," 'transformer.h.1.attn.c_attn.bias': tensor(65.4315, device='cuda:0'),\n"," 'transformer.h.1.attn.c_proj.weight': tensor(27.0571, device='cuda:0'),\n"," 'transformer.h.1.attn.c_proj.bias': tensor(101.8587, device='cuda:0'),\n"," 'transformer.h.1.ln_2.weight': tensor(281.8196, device='cuda:0'),\n"," 'transformer.h.1.ln_2.bias': tensor(216.0666, device='cuda:0'),\n"," 'transformer.h.1.mlp.c_fc.weight': tensor(55.6911, device='cuda:0'),\n"," 'transformer.h.1.mlp.c_fc.bias': tensor(193.1642, device='cuda:0'),\n"," 'transformer.h.1.mlp.c_proj.weight': tensor(9.2751, device='cuda:0'),\n"," 'transformer.h.1.mlp.c_proj.bias': tensor(79.6125, device='cuda:0'),\n"," 'transformer.h.2.ln_1.weight': tensor(134.1642, device='cuda:0'),\n"," 'transformer.h.2.ln_1.bias': tensor(116.0916, device='cuda:0'),\n"," 'transformer.h.2.attn.c_attn.weight': tensor(76.0895, device='cuda:0'),\n"," 'transformer.h.2.attn.c_attn.bias': tensor(86.8576, device='cuda:0'),\n"," 'transformer.h.2.attn.c_proj.weight': tensor(55.3764, device='cuda:0'),\n"," 'transformer.h.2.attn.c_proj.bias': tensor(246.7206, device='cuda:0'),\n"," 'transformer.h.2.ln_2.weight': tensor(177.0001, device='cuda:0'),\n"," 'transformer.h.2.ln_2.bias': tensor(196.4717, device='cuda:0'),\n"," 'transformer.h.2.mlp.c_fc.weight': tensor(12.0679, device='cuda:0'),\n"," 'transformer.h.2.mlp.c_fc.bias': tensor(73.9293, device='cuda:0'),\n"," 'transformer.h.2.mlp.c_proj.weight': tensor(8.4678, device='cuda:0'),\n"," 'transformer.h.2.mlp.c_proj.bias': tensor(81.1532, device='cuda:0'),\n"," 'transformer.h.3.ln_1.weight': tensor(165.6309, device='cuda:0'),\n"," 'transformer.h.3.ln_1.bias': tensor(73.0905, device='cuda:0'),\n"," 'transformer.h.3.attn.c_attn.weight': tensor(67.2003, device='cuda:0'),\n"," 'transformer.h.3.attn.c_attn.bias': tensor(178.3555, device='cuda:0'),\n"," 'transformer.h.3.attn.c_proj.weight': tensor(60.8180, device='cuda:0'),\n"," 'transformer.h.3.attn.c_proj.bias': tensor(123.5897, device='cuda:0'),\n"," 'transformer.h.3.ln_2.weight': tensor(110.1740, device='cuda:0'),\n"," 'transformer.h.3.ln_2.bias': tensor(289.7144, device='cuda:0'),\n"," 'transformer.h.3.mlp.c_fc.weight': tensor(48.0373, device='cuda:0'),\n"," 'transformer.h.3.mlp.c_fc.bias': tensor(102.7957, device='cuda:0'),\n"," 'transformer.h.3.mlp.c_proj.weight': tensor(7.4539, device='cuda:0'),\n"," 'transformer.h.3.mlp.c_proj.bias': tensor(68.2286, device='cuda:0'),\n"," 'transformer.h.4.ln_1.weight': tensor(189.5896, device='cuda:0'),\n"," 'transformer.h.4.ln_1.bias': tensor(81.8323, device='cuda:0'),\n"," 'transformer.h.4.attn.c_attn.weight': tensor(38.2450, device='cuda:0'),\n"," 'transformer.h.4.attn.c_attn.bias': tensor(46.2161, device='cuda:0'),\n"," 'transformer.h.4.attn.c_proj.weight': tensor(69.1690, device='cuda:0'),\n"," 'transformer.h.4.attn.c_proj.bias': tensor(197.9954, device='cuda:0'),\n"," 'transformer.h.4.ln_2.weight': tensor(113.1744, device='cuda:0'),\n"," 'transformer.h.4.ln_2.bias': tensor(896.3248, device='cuda:0'),\n"," 'transformer.h.4.mlp.c_fc.weight': tensor(58.9788, device='cuda:0'),\n"," 'transformer.h.4.mlp.c_fc.bias': tensor(171.1866, device='cuda:0'),\n"," 'transformer.h.4.mlp.c_proj.weight': tensor(26.6624, device='cuda:0'),\n"," 'transformer.h.4.mlp.c_proj.bias': tensor(81.1859, device='cuda:0'),\n"," 'transformer.h.5.ln_1.weight': tensor(164.1738, device='cuda:0'),\n"," 'transformer.h.5.ln_1.bias': tensor(117.6082, device='cuda:0'),\n"," 'transformer.h.5.attn.c_attn.weight': tensor(91.5912, device='cuda:0'),\n"," 'transformer.h.5.attn.c_attn.bias': tensor(226.9202, device='cuda:0'),\n"," 'transformer.h.5.attn.c_proj.weight': tensor(64.3864, device='cuda:0'),\n"," 'transformer.h.5.attn.c_proj.bias': tensor(181.5699, device='cuda:0'),\n"," 'transformer.h.5.ln_2.weight': tensor(90.1438, device='cuda:0'),\n"," 'transformer.h.5.ln_2.bias': tensor(399.7993, device='cuda:0'),\n"," 'transformer.h.5.mlp.c_fc.weight': tensor(64.8294, device='cuda:0'),\n"," 'transformer.h.5.mlp.c_fc.bias': tensor(189.1982, device='cuda:0'),\n"," 'transformer.h.5.mlp.c_proj.weight': tensor(44.9104, device='cuda:0'),\n"," 'transformer.h.5.mlp.c_proj.bias': tensor(101.6104, device='cuda:0'),\n"," 'transformer.h.6.ln_1.weight': tensor(161.1443, device='cuda:0'),\n"," 'transformer.h.6.ln_1.bias': tensor(83.4212, device='cuda:0'),\n"," 'transformer.h.6.attn.c_attn.weight': tensor(78.3503, device='cuda:0'),\n"," 'transformer.h.6.attn.c_attn.bias': tensor(155.7361, device='cuda:0'),\n"," 'transformer.h.6.attn.c_proj.weight': tensor(68.3638, device='cuda:0'),\n"," 'transformer.h.6.attn.c_proj.bias': tensor(322.2487, device='cuda:0'),\n"," 'transformer.h.6.ln_2.weight': tensor(95.3251, device='cuda:0'),\n"," 'transformer.h.6.ln_2.bias': tensor(281.3669, device='cuda:0'),\n"," 'transformer.h.6.mlp.c_fc.weight': tensor(59.8931, device='cuda:0'),\n"," 'transformer.h.6.mlp.c_fc.bias': tensor(185.7360, device='cuda:0'),\n"," 'transformer.h.6.mlp.c_proj.weight': tensor(45.9970, device='cuda:0'),\n"," 'transformer.h.6.mlp.c_proj.bias': tensor(121.7020, device='cuda:0'),\n"," 'transformer.h.7.ln_1.weight': tensor(154.3819, device='cuda:0'),\n"," 'transformer.h.7.ln_1.bias': tensor(106.1733, device='cuda:0'),\n"," 'transformer.h.7.attn.c_attn.weight': tensor(70.6231, device='cuda:0'),\n"," 'transformer.h.7.attn.c_attn.bias': tensor(167.4841, device='cuda:0'),\n"," 'transformer.h.7.attn.c_proj.weight': tensor(57.2942, device='cuda:0'),\n"," 'transformer.h.7.attn.c_proj.bias': tensor(246.3623, device='cuda:0'),\n"," 'transformer.h.7.ln_2.weight': tensor(98.7808, device='cuda:0'),\n"," 'transformer.h.7.ln_2.bias': tensor(202.4408, device='cuda:0'),\n"," 'transformer.h.7.mlp.c_fc.weight': tensor(103.5272, device='cuda:0'),\n"," 'transformer.h.7.mlp.c_fc.bias': tensor(152.4989, device='cuda:0'),\n"," 'transformer.h.7.mlp.c_proj.weight': tensor(28.2108, device='cuda:0'),\n"," 'transformer.h.7.mlp.c_proj.bias': tensor(108.4600, device='cuda:0'),\n"," 'transformer.h.8.ln_1.weight': tensor(136.0485, device='cuda:0'),\n"," 'transformer.h.8.ln_1.bias': tensor(87.3525, device='cuda:0'),\n"," 'transformer.h.8.attn.c_attn.weight': tensor(65.4546, device='cuda:0'),\n"," 'transformer.h.8.attn.c_attn.bias': tensor(145.6444, device='cuda:0'),\n"," 'transformer.h.8.attn.c_proj.weight': tensor(43.3395, device='cuda:0'),\n"," 'transformer.h.8.attn.c_proj.bias': tensor(109.7932, device='cuda:0'),\n"," 'transformer.h.8.ln_2.weight': tensor(119.2711, device='cuda:0'),\n"," 'transformer.h.8.ln_2.bias': tensor(182.5739, device='cuda:0'),\n"," 'transformer.h.8.mlp.c_fc.weight': tensor(87.3688, device='cuda:0'),\n"," 'transformer.h.8.mlp.c_fc.bias': tensor(127.8743, device='cuda:0'),\n"," 'transformer.h.8.mlp.c_proj.weight': tensor(23.7382, device='cuda:0'),\n"," 'transformer.h.8.mlp.c_proj.bias': tensor(104.5344, device='cuda:0'),\n"," 'transformer.h.9.ln_1.weight': tensor(132.8354, device='cuda:0'),\n"," 'transformer.h.9.ln_1.bias': tensor(100.4733, device='cuda:0'),\n"," 'transformer.h.9.attn.c_attn.weight': tensor(64.1534, device='cuda:0'),\n"," 'transformer.h.9.attn.c_attn.bias': tensor(121.6672, device='cuda:0'),\n"," 'transformer.h.9.attn.c_proj.weight': tensor(64.4294, device='cuda:0'),\n"," 'transformer.h.9.attn.c_proj.bias': tensor(67.0744, device='cuda:0'),\n"," 'transformer.h.9.ln_2.weight': tensor(135.1793, device='cuda:0'),\n"," 'transformer.h.9.ln_2.bias': tensor(226.4996, device='cuda:0'),\n"," 'transformer.h.9.mlp.c_fc.weight': tensor(45.8675, device='cuda:0'),\n"," 'transformer.h.9.mlp.c_fc.bias': tensor(208.7671, device='cuda:0'),\n"," 'transformer.h.9.mlp.c_proj.weight': tensor(23.2572, device='cuda:0'),\n"," 'transformer.h.9.mlp.c_proj.bias': tensor(85.7343, device='cuda:0'),\n"," 'transformer.h.10.ln_1.weight': tensor(136.1570, device='cuda:0'),\n"," 'transformer.h.10.ln_1.bias': tensor(116.5597, device='cuda:0'),\n"," 'transformer.h.10.attn.c_attn.weight': tensor(67.2551, device='cuda:0'),\n"," 'transformer.h.10.attn.c_attn.bias': tensor(138.6725, device='cuda:0'),\n"," 'transformer.h.10.attn.c_proj.weight': tensor(30.2380, device='cuda:0'),\n"," 'transformer.h.10.attn.c_proj.bias': tensor(33.0275, device='cuda:0'),\n"," 'transformer.h.10.ln_2.weight': tensor(116.1319, device='cuda:0'),\n"," 'transformer.h.10.ln_2.bias': tensor(184.0614, device='cuda:0'),\n"," 'transformer.h.10.mlp.c_fc.weight': tensor(49.7241, device='cuda:0'),\n"," 'transformer.h.10.mlp.c_fc.bias': tensor(119.3500, device='cuda:0'),\n"," 'transformer.h.10.mlp.c_proj.weight': tensor(11.5674, device='cuda:0'),\n"," 'transformer.h.10.mlp.c_proj.bias': tensor(94.4374, device='cuda:0'),\n"," 'transformer.h.11.ln_1.weight': tensor(132.9598, device='cuda:0'),\n"," 'transformer.h.11.ln_1.bias': tensor(126.3241, device='cuda:0'),\n"," 'transformer.h.11.attn.c_attn.weight': tensor(54.9319, device='cuda:0'),\n"," 'transformer.h.11.attn.c_attn.bias': tensor(156.6264, device='cuda:0'),\n"," 'transformer.h.11.attn.c_proj.weight': tensor(14.3364, device='cuda:0'),\n"," 'transformer.h.11.attn.c_proj.bias': tensor(23.6097, device='cuda:0'),\n"," 'transformer.h.11.ln_2.weight': tensor(102.2381, device='cuda:0'),\n"," 'transformer.h.11.ln_2.bias': tensor(304.4107, device='cuda:0'),\n"," 'transformer.h.11.mlp.c_fc.weight': tensor(65.3686, device='cuda:0'),\n"," 'transformer.h.11.mlp.c_fc.bias': tensor(103.3926, device='cuda:0'),\n"," 'transformer.h.11.mlp.c_proj.weight': tensor(13.8765, device='cuda:0'),\n"," 'transformer.h.11.mlp.c_proj.bias': tensor(294.2222, device='cuda:0')}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# trial run of post quantization inference\n","\n","model.quantize = True\n","torch.cuda.reset_max_memory_allocated()\n","\n","start_time = time.time()\n","y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","print(decode(y[0].tolist()))\n","print('---------------')\n","\n","print(f\"Took {time.time() - start_time} seconds\")\n","print(f\"Memory used for inference with quantization: {torch.cuda.max_memory_allocated()} bytes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-aZXT_aX9TrQ","executionInfo":{"status":"ok","timestamp":1701821116309,"user_tz":480,"elapsed":22913,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"cb1f3474-7f21-40d3-c4d4-3fc4d1393a59"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","- * * AllMusic \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = Historiography and studies = = \n","<|endoftext|> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n","---------------\n","Took 22.83383321762085 seconds\n","Memory used for inference with quantization: 1838864896 bytes\n"]}]},{"cell_type":"code","source":["state_dict = model.state_dict()\n","total_bytes = 0\n","\n","for key, tensor in state_dict.items():\n","    total_bytes += tensor.numel() * tensor.element_size()\n","\n","print(f\"Total memory usage of state_dict in bytes: {total_bytes}\")\n","print(f\"Total memory usage of state_dict in kilobytes: {total_bytes / 1024}\")\n","print(f\"Total memory usage of state_dict in megabytes: {total_bytes / (1024 ** 2)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pib3saeZEeEI","executionInfo":{"status":"ok","timestamp":1701821116309,"user_tz":480,"elapsed":10,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"671a9787-f004-4444-bd38-2b6ba5887185"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Total memory usage of state_dict in bytes: 396985344\n","Total memory usage of state_dict in kilobytes: 387681.0\n","Total memory usage of state_dict in megabytes: 378.5947265625\n"]}]},{"cell_type":"code","source":["quantize_checkpoint = {\n","    'model': model.state_dict(),\n","    'model_args': checkpoint['model_args'],\n","    'quantization_scales' : model.quantization_scales\n","}\n","torch.save(quantize_checkpoint, os.path.join(out_dir, 'quantized_ckpt.pt'))"],"metadata":{"id":"QJfTpS1f9HCV","executionInfo":{"status":"ok","timestamp":1701821119358,"user_tz":480,"elapsed":3058,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## Quantized Model Testing"],"metadata":{"id":"pC8h4vPs6XlC"}},{"cell_type":"code","source":["import tiktoken\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'wikitext' # ignored if init_from is not 'resume'\n","start = \"Say something cool:\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# initialize quantized.model\n","# init from a model saved in a specific directory\n","ckpt_path = os.path.join(out_dir, 'quantized_ckpt.pt')\n","checkpoint = torch.load(ckpt_path, map_location=device)\n","gptconf = GPTConfig(**checkpoint['model_args'])\n","model = GPT(gptconf)\n","state_dict = checkpoint['model']\n","unwanted_prefix = '_orig_mod.'\n","# format params\n","for k,v in list(state_dict.items()):\n","    if k.startswith(unwanted_prefix):\n","        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","# add params\n","for name, param in model.named_parameters():\n","    if 'transformer.h' in name:\n","      param.requires_grad = False\n","      param.data = param.data.to(torch.int8)\n","model.load_state_dict(state_dict)\n","# add quantization scales\n","model.quantization_scales = checkpoint['quantization_scales']\n","print(model.quantization_scales)\n","\n","\n","\n","# switch to eval\n","model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n","\n","# ok let's assume gpt-2 encodings by default\n","print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n","enc = tiktoken.get_encoding(\"gpt2\")\n","encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","decode = lambda l: enc.decode(l)\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25gjsB50RA9K","executionInfo":{"status":"ok","timestamp":1701822197333,"user_tz":480,"elapsed":3406,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"26ceba75-14e0-4104-ea3f-43d7075c738b"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model in quantized mode\n","number of parameters: 123.65M\n","{'transformer.h.0.ln_1.weight': tensor(501.8847, device='cuda:0'), 'transformer.h.0.ln_1.bias': tensor(490.1018, device='cuda:0'), 'transformer.h.0.attn.c_attn.weight': tensor(44.7227, device='cuda:0'), 'transformer.h.0.attn.c_attn.bias': tensor(94.9118, device='cuda:0'), 'transformer.h.0.attn.c_proj.weight': tensor(38.4227, device='cuda:0'), 'transformer.h.0.attn.c_proj.bias': tensor(47.3122, device='cuda:0'), 'transformer.h.0.ln_2.weight': tensor(83.9078, device='cuda:0'), 'transformer.h.0.ln_2.bias': tensor(171.3022, device='cuda:0'), 'transformer.h.0.mlp.c_fc.weight': tensor(27.8004, device='cuda:0'), 'transformer.h.0.mlp.c_fc.bias': tensor(170.8822, device='cuda:0'), 'transformer.h.0.mlp.c_proj.weight': tensor(20.7393, device='cuda:0'), 'transformer.h.0.mlp.c_proj.bias': tensor(85.9377, device='cuda:0'), 'transformer.h.1.ln_1.weight': tensor(192.7835, device='cuda:0'), 'transformer.h.1.ln_1.bias': tensor(190.9859, device='cuda:0'), 'transformer.h.1.attn.c_attn.weight': tensor(103.1711, device='cuda:0'), 'transformer.h.1.attn.c_attn.bias': tensor(65.4315, device='cuda:0'), 'transformer.h.1.attn.c_proj.weight': tensor(27.0571, device='cuda:0'), 'transformer.h.1.attn.c_proj.bias': tensor(101.8587, device='cuda:0'), 'transformer.h.1.ln_2.weight': tensor(281.8196, device='cuda:0'), 'transformer.h.1.ln_2.bias': tensor(216.0666, device='cuda:0'), 'transformer.h.1.mlp.c_fc.weight': tensor(55.6911, device='cuda:0'), 'transformer.h.1.mlp.c_fc.bias': tensor(193.1642, device='cuda:0'), 'transformer.h.1.mlp.c_proj.weight': tensor(9.2751, device='cuda:0'), 'transformer.h.1.mlp.c_proj.bias': tensor(79.6125, device='cuda:0'), 'transformer.h.2.ln_1.weight': tensor(134.1642, device='cuda:0'), 'transformer.h.2.ln_1.bias': tensor(116.0916, device='cuda:0'), 'transformer.h.2.attn.c_attn.weight': tensor(76.0895, device='cuda:0'), 'transformer.h.2.attn.c_attn.bias': tensor(86.8576, device='cuda:0'), 'transformer.h.2.attn.c_proj.weight': tensor(55.3764, device='cuda:0'), 'transformer.h.2.attn.c_proj.bias': tensor(246.7206, device='cuda:0'), 'transformer.h.2.ln_2.weight': tensor(177.0001, device='cuda:0'), 'transformer.h.2.ln_2.bias': tensor(196.4717, device='cuda:0'), 'transformer.h.2.mlp.c_fc.weight': tensor(12.0679, device='cuda:0'), 'transformer.h.2.mlp.c_fc.bias': tensor(73.9293, device='cuda:0'), 'transformer.h.2.mlp.c_proj.weight': tensor(8.4678, device='cuda:0'), 'transformer.h.2.mlp.c_proj.bias': tensor(81.1532, device='cuda:0'), 'transformer.h.3.ln_1.weight': tensor(165.6309, device='cuda:0'), 'transformer.h.3.ln_1.bias': tensor(73.0905, device='cuda:0'), 'transformer.h.3.attn.c_attn.weight': tensor(67.2003, device='cuda:0'), 'transformer.h.3.attn.c_attn.bias': tensor(178.3555, device='cuda:0'), 'transformer.h.3.attn.c_proj.weight': tensor(60.8180, device='cuda:0'), 'transformer.h.3.attn.c_proj.bias': tensor(123.5897, device='cuda:0'), 'transformer.h.3.ln_2.weight': tensor(110.1740, device='cuda:0'), 'transformer.h.3.ln_2.bias': tensor(289.7144, device='cuda:0'), 'transformer.h.3.mlp.c_fc.weight': tensor(48.0373, device='cuda:0'), 'transformer.h.3.mlp.c_fc.bias': tensor(102.7957, device='cuda:0'), 'transformer.h.3.mlp.c_proj.weight': tensor(7.4539, device='cuda:0'), 'transformer.h.3.mlp.c_proj.bias': tensor(68.2286, device='cuda:0'), 'transformer.h.4.ln_1.weight': tensor(189.5896, device='cuda:0'), 'transformer.h.4.ln_1.bias': tensor(81.8323, device='cuda:0'), 'transformer.h.4.attn.c_attn.weight': tensor(38.2450, device='cuda:0'), 'transformer.h.4.attn.c_attn.bias': tensor(46.2161, device='cuda:0'), 'transformer.h.4.attn.c_proj.weight': tensor(69.1690, device='cuda:0'), 'transformer.h.4.attn.c_proj.bias': tensor(197.9954, device='cuda:0'), 'transformer.h.4.ln_2.weight': tensor(113.1744, device='cuda:0'), 'transformer.h.4.ln_2.bias': tensor(896.3248, device='cuda:0'), 'transformer.h.4.mlp.c_fc.weight': tensor(58.9788, device='cuda:0'), 'transformer.h.4.mlp.c_fc.bias': tensor(171.1866, device='cuda:0'), 'transformer.h.4.mlp.c_proj.weight': tensor(26.6624, device='cuda:0'), 'transformer.h.4.mlp.c_proj.bias': tensor(81.1859, device='cuda:0'), 'transformer.h.5.ln_1.weight': tensor(164.1738, device='cuda:0'), 'transformer.h.5.ln_1.bias': tensor(117.6082, device='cuda:0'), 'transformer.h.5.attn.c_attn.weight': tensor(91.5912, device='cuda:0'), 'transformer.h.5.attn.c_attn.bias': tensor(226.9202, device='cuda:0'), 'transformer.h.5.attn.c_proj.weight': tensor(64.3864, device='cuda:0'), 'transformer.h.5.attn.c_proj.bias': tensor(181.5699, device='cuda:0'), 'transformer.h.5.ln_2.weight': tensor(90.1438, device='cuda:0'), 'transformer.h.5.ln_2.bias': tensor(399.7993, device='cuda:0'), 'transformer.h.5.mlp.c_fc.weight': tensor(64.8294, device='cuda:0'), 'transformer.h.5.mlp.c_fc.bias': tensor(189.1982, device='cuda:0'), 'transformer.h.5.mlp.c_proj.weight': tensor(44.9104, device='cuda:0'), 'transformer.h.5.mlp.c_proj.bias': tensor(101.6104, device='cuda:0'), 'transformer.h.6.ln_1.weight': tensor(161.1443, device='cuda:0'), 'transformer.h.6.ln_1.bias': tensor(83.4212, device='cuda:0'), 'transformer.h.6.attn.c_attn.weight': tensor(78.3503, device='cuda:0'), 'transformer.h.6.attn.c_attn.bias': tensor(155.7361, device='cuda:0'), 'transformer.h.6.attn.c_proj.weight': tensor(68.3638, device='cuda:0'), 'transformer.h.6.attn.c_proj.bias': tensor(322.2487, device='cuda:0'), 'transformer.h.6.ln_2.weight': tensor(95.3251, device='cuda:0'), 'transformer.h.6.ln_2.bias': tensor(281.3669, device='cuda:0'), 'transformer.h.6.mlp.c_fc.weight': tensor(59.8931, device='cuda:0'), 'transformer.h.6.mlp.c_fc.bias': tensor(185.7360, device='cuda:0'), 'transformer.h.6.mlp.c_proj.weight': tensor(45.9970, device='cuda:0'), 'transformer.h.6.mlp.c_proj.bias': tensor(121.7020, device='cuda:0'), 'transformer.h.7.ln_1.weight': tensor(154.3819, device='cuda:0'), 'transformer.h.7.ln_1.bias': tensor(106.1733, device='cuda:0'), 'transformer.h.7.attn.c_attn.weight': tensor(70.6231, device='cuda:0'), 'transformer.h.7.attn.c_attn.bias': tensor(167.4841, device='cuda:0'), 'transformer.h.7.attn.c_proj.weight': tensor(57.2942, device='cuda:0'), 'transformer.h.7.attn.c_proj.bias': tensor(246.3623, device='cuda:0'), 'transformer.h.7.ln_2.weight': tensor(98.7808, device='cuda:0'), 'transformer.h.7.ln_2.bias': tensor(202.4408, device='cuda:0'), 'transformer.h.7.mlp.c_fc.weight': tensor(103.5272, device='cuda:0'), 'transformer.h.7.mlp.c_fc.bias': tensor(152.4989, device='cuda:0'), 'transformer.h.7.mlp.c_proj.weight': tensor(28.2108, device='cuda:0'), 'transformer.h.7.mlp.c_proj.bias': tensor(108.4600, device='cuda:0'), 'transformer.h.8.ln_1.weight': tensor(136.0485, device='cuda:0'), 'transformer.h.8.ln_1.bias': tensor(87.3525, device='cuda:0'), 'transformer.h.8.attn.c_attn.weight': tensor(65.4546, device='cuda:0'), 'transformer.h.8.attn.c_attn.bias': tensor(145.6444, device='cuda:0'), 'transformer.h.8.attn.c_proj.weight': tensor(43.3395, device='cuda:0'), 'transformer.h.8.attn.c_proj.bias': tensor(109.7932, device='cuda:0'), 'transformer.h.8.ln_2.weight': tensor(119.2711, device='cuda:0'), 'transformer.h.8.ln_2.bias': tensor(182.5739, device='cuda:0'), 'transformer.h.8.mlp.c_fc.weight': tensor(87.3688, device='cuda:0'), 'transformer.h.8.mlp.c_fc.bias': tensor(127.8743, device='cuda:0'), 'transformer.h.8.mlp.c_proj.weight': tensor(23.7382, device='cuda:0'), 'transformer.h.8.mlp.c_proj.bias': tensor(104.5344, device='cuda:0'), 'transformer.h.9.ln_1.weight': tensor(132.8354, device='cuda:0'), 'transformer.h.9.ln_1.bias': tensor(100.4733, device='cuda:0'), 'transformer.h.9.attn.c_attn.weight': tensor(64.1534, device='cuda:0'), 'transformer.h.9.attn.c_attn.bias': tensor(121.6672, device='cuda:0'), 'transformer.h.9.attn.c_proj.weight': tensor(64.4294, device='cuda:0'), 'transformer.h.9.attn.c_proj.bias': tensor(67.0744, device='cuda:0'), 'transformer.h.9.ln_2.weight': tensor(135.1793, device='cuda:0'), 'transformer.h.9.ln_2.bias': tensor(226.4996, device='cuda:0'), 'transformer.h.9.mlp.c_fc.weight': tensor(45.8675, device='cuda:0'), 'transformer.h.9.mlp.c_fc.bias': tensor(208.7671, device='cuda:0'), 'transformer.h.9.mlp.c_proj.weight': tensor(23.2572, device='cuda:0'), 'transformer.h.9.mlp.c_proj.bias': tensor(85.7343, device='cuda:0'), 'transformer.h.10.ln_1.weight': tensor(136.1570, device='cuda:0'), 'transformer.h.10.ln_1.bias': tensor(116.5597, device='cuda:0'), 'transformer.h.10.attn.c_attn.weight': tensor(67.2551, device='cuda:0'), 'transformer.h.10.attn.c_attn.bias': tensor(138.6725, device='cuda:0'), 'transformer.h.10.attn.c_proj.weight': tensor(30.2380, device='cuda:0'), 'transformer.h.10.attn.c_proj.bias': tensor(33.0275, device='cuda:0'), 'transformer.h.10.ln_2.weight': tensor(116.1319, device='cuda:0'), 'transformer.h.10.ln_2.bias': tensor(184.0614, device='cuda:0'), 'transformer.h.10.mlp.c_fc.weight': tensor(49.7241, device='cuda:0'), 'transformer.h.10.mlp.c_fc.bias': tensor(119.3500, device='cuda:0'), 'transformer.h.10.mlp.c_proj.weight': tensor(11.5674, device='cuda:0'), 'transformer.h.10.mlp.c_proj.bias': tensor(94.4374, device='cuda:0'), 'transformer.h.11.ln_1.weight': tensor(132.9598, device='cuda:0'), 'transformer.h.11.ln_1.bias': tensor(126.3241, device='cuda:0'), 'transformer.h.11.attn.c_attn.weight': tensor(54.9319, device='cuda:0'), 'transformer.h.11.attn.c_attn.bias': tensor(156.6264, device='cuda:0'), 'transformer.h.11.attn.c_proj.weight': tensor(14.3364, device='cuda:0'), 'transformer.h.11.attn.c_proj.bias': tensor(23.6097, device='cuda:0'), 'transformer.h.11.ln_2.weight': tensor(102.2381, device='cuda:0'), 'transformer.h.11.ln_2.bias': tensor(304.4107, device='cuda:0'), 'transformer.h.11.mlp.c_fc.weight': tensor(65.3686, device='cuda:0'), 'transformer.h.11.mlp.c_fc.bias': tensor(103.3926, device='cuda:0'), 'transformer.h.11.mlp.c_proj.weight': tensor(13.8765, device='cuda:0'), 'transformer.h.11.mlp.c_proj.bias': tensor(294.2222, device='cuda:0')}\n","No meta.pkl found, assuming GPT-2 encodings...\n"]}]},{"cell_type":"code","source":["model.quantize = True\n","torch.cuda.reset_max_memory_allocated()\n","\n","start_time = time.time()\n","y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","print(decode(y[0].tolist()))\n","print('---------------')\n","\n","print(f\"Took {time.time() - start_time} seconds\")\n","print(f\"Memory used for inference with quantization: {torch.cuda.max_memory_allocated()} bytes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-LO_NfGDFBi","executionInfo":{"status":"ok","timestamp":1701822223220,"user_tz":480,"elapsed":21695,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"aa97d45d-38ea-4af5-b0be-139445b083ad"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Say something cool: \" * \" All four \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = = = = = 2013 : A. E. W. Johnson = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n","---------------\n","Took 21.52263307571411 seconds\n","Memory used for inference with quantization: 1082324992 bytes\n"]}]},{"cell_type":"markdown","source":["## Perplexity and Memory Usage"],"metadata":{"id":"pCe0Jl4KLbOk"}},{"cell_type":"code","source":[],"metadata":{"id":"iTmZLZ1lLa4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Speculative Decoding"],"metadata":{"id":"aoAvbIjt6aMC"}}]}