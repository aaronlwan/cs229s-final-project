{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNQyck4IXSmNoFBWXWj+pg+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Imports and Google Drive Auth"],"metadata":{"id":"yw4hozax6Kmr"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SwCvckossDr","executionInfo":{"status":"ok","timestamp":1701825740867,"user_tz":480,"elapsed":12634,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"d76e98a0-b435-4239-99df-1697ebe9240b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, pyarrow-hotfix, docker-pycreds, dill, tiktoken, multiprocess, gitdb, GitPython, wandb, datasets\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.40 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 gitdb-4.0.11 multiprocess-0.70.15 pyarrow-hotfix-0.6 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 tiktoken-0.5.2 wandb-0.16.1\n"]}],"source":["!pip install torch numpy transformers datasets tiktoken wandb tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaH__RGYs_xi","executionInfo":{"status":"ok","timestamp":1701825758305,"user_tz":480,"elapsed":17442,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"c3b8c8dd-5609-4522-e2aa-0f8feae342e2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Fall/CS229S/cs229s-final-project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47xHdkM2s_nX","executionInfo":{"status":"ok","timestamp":1701825758684,"user_tz":480,"elapsed":383,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"d1b9c8ec-1241-4965-bbe7-9b97592dcdcc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Fall/CS229S/cs229s-final-project\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","from model import GPTConfig, GPT\n","\n","import tiktoken"],"metadata":{"id":"JL1KXKlOtCMx","executionInfo":{"status":"ok","timestamp":1701825765050,"user_tz":480,"elapsed":6367,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Model Init and Query"],"metadata":{"id":"xT1RzZ6ZfaG5"}},{"cell_type":"code","source":["import tiktoken\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'wikitext' # ignored if init_from is not 'resume'\n","start = \"Say something cool:\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    print(f'Loading from {ckpt_path}')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","elif init_from.startswith('gpt2'):\n","    # init from a given GPT-2 model\n","    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n","\n","model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n","\n","# look for the meta pickle in case it is available in the dataset folder\n","load_meta = False\n","if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n","    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n","    load_meta = os.path.exists(meta_path)\n","if load_meta:\n","    print(f\"Loading meta from {meta_path}...\")\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    # TODO want to make this more general to arbitrary encoder/decoder schemes\n","    stoi, itos = meta['stoi'], meta['itos']\n","    encode = lambda s: [stoi[c] for c in s]\n","    decode = lambda l: ''.join([itos[i] for i in l])\n","else:\n","    # ok let's assume gpt-2 encodings by default\n","    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n","    enc = tiktoken.get_encoding(\"gpt2\")\n","    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n","    decode = lambda l: enc.decode(l)\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6u1CqDqdfZ7M","executionInfo":{"status":"ok","timestamp":1701825796027,"user_tz":480,"elapsed":30980,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"c1d0a34a-48f2-4baa-bbe2-173550c457bc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading from wikitext/ckpt.pt\n","Model in quantized mode\n","number of parameters: 123.65M\n","No meta.pkl found, assuming GPT-2 encodings...\n"]}]},{"cell_type":"code","source":["model.quantize = False\n","torch.cuda.reset_max_memory_allocated()\n","\n","start_time = time.time()\n","y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","print(decode(y[0].tolist()))\n","print('---------------')\n","\n","print(f\"Took {time.time() - start_time} seconds\")\n","print(f\"Memory used for inference with quantization: {torch.cuda.max_memory_allocated()} bytes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITlbEy9hfkOX","executionInfo":{"status":"ok","timestamp":1701825806727,"user_tz":480,"elapsed":10704,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"46aedef2-c2b2-4d53-f75e-819b945416c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Say something cool: ' Show some respect to the world ' ' \n","<|endoftext|><|endoftext|><|endoftext|> = = = Postwar = = = \n","<|endoftext|><|endoftext|> = = = = = East Coast = = = = = \n","<|endoftext|> = = Cast = = \n","<|endoftext|> = = = Musical = = = \n","<|endoftext|><|endoftext|> = = = Critical reception = = = \n","<|endoftext|><|endoftext|> \" This is the best thing I 've ever believed in my whole life , \" said the actor , whose experience with The Lord of the Rings made him consider the film a \" real film \" , although he had always considered it to be a \" bad , ugly movie \" . He also believed that the film could \" be seen as a cinematic nightmare \" . Davies stated that , because the film was dark at the time due to its presentation of the myths and legends of the Middle Ages , the audience would be hard pressed to make out the scenes depicted in the film . \n","<|endoftext|> = = Order of battle = = \n","<|endoftext|> The original version of the song was recorded at Beach World Beach in <unk> , Maryland and released for download as the instrumental version of the original film soundtrack for the 1993 film , The Go @-@ Go Man . \" I Want You 're Here Today \" was the first track of the soundtrack . \" It 's very interesting , \" Als said . \" I just felt like I had to do it . It 's this funky , strange rhythmic sound . It 's a really good blend of the song with all the other stuff and I think it 's definitely a great song . It was so pure and everything in the film was inspired by the song . \" Michael Jackson – The Motion Picture Composer ( track ) \n","<|endoftext|> = = Reception = = \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = Early career = = \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> , M. D. ( 2008 ) , The Law of Attraction as Explanatory Fiction : The Law of Attraction and its Applications in Contemporary Literature . Springer . ISBN 0 @-@ <unk> @-@ 1481 @-@ 6 . \n","<|endoftext|><|endoftext|> = = Construction = = \n","<|endoftext|><|endoftext|> = = = = 2004 = = = = \n","<|endoftext|><|endoftext|><|endoftext|> = = = Critical response = = = \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = = Home media box office = = = \n","<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> = = = Rivalries ( 1978 – 1983 ) =\n","---------------\n","Took 10.598570346832275 seconds\n","Memory used for inference with quantization: 2039453184 bytes\n"]}]},{"cell_type":"markdown","source":["## Perplexity and Memory Usage"],"metadata":{"id":"TV9gl1kwfclU"}},{"cell_type":"code","source":["out_dir = 'out'\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False # if True, script exits right after the first eval\n","always_save_checkpoint = True # if True, always save a checkpoint after each eval\n","init_from = 'gpt2' # 'scratch' or 'resume' or 'gpt2*'\n","# wandb logging\n","wandb_log = False # disabled by default\n","wandb_project = 'cs229s'\n","wandb_run_name = 'gpt2' # 'run' + str(time.time())\n","# data\n","dataset = 'wikitext'\n","gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n","batch_size = 8 # if gradient_accumulation_steps > 1, this is the micro-batch size\n","block_size = 1024\n","# model\n","n_layer = 12\n","n_head = 12\n","n_embd = 768\n","dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n","bias = False # do we use bias inside LayerNorm and Linear layers?\n","# adamw optimizer\n","learning_rate = 6e-4 # max learning rate\n","max_iters = 600000 # total number of training iterations\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n","# learning rate decay settings\n","decay_lr = True # whether to decay the learning rate\n","warmup_iters = 2000 # how many steps to warm up for\n","lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n","min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n","# DDP settings\n","backend = 'nccl' # 'nccl', 'gloo', etc.\n","# system\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","# exec(open('configurator.py').read()) # overrides from command line or config file\n","wandb_log = True\n","wandb_project = 'wikitext'\n","wandb_run_name='gpt2-124M'\n","\n","# checkpoint\n","out_dir = 'wikitext'\n","\n","# these make the total batch size be ~0.5M\n","# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520\n","batch_size = 8\n","block_size = 1024\n","gradient_accumulation_steps = 5 * 8\n","\n","# this makes total number of tokens be 300B\n","max_iters = 600000\n","lr_decay_iters = 600000\n","\n","# eval stuff\n","eval_interval = 50\n","eval_iters = 100\n","log_interval = 10\n","\n","# weight decay\n","weight_decay = 1e-1\n","config = {k: globals()[k] for k in config_keys} # will be useful for logging\n","# -----------------------------------------------------------------------------\n","\n","ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n","if ddp:\n","    init_process_group(backend=backend)\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'cuda:{ddp_local_rank}'\n","    torch.cuda.set_device(device)\n","    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n","    seed_offset = ddp_rank # each process gets a different seed\n","    # world_size number of processes will be training simultaneously, so we can scale\n","    # down the desired gradient accumulation iterations per process proportionally\n","    assert gradient_accumulation_steps % ddp_world_size == 0\n","    gradient_accumulation_steps //= ddp_world_size\n","else:\n","    # if not ddp, we are running on a single gpu, and one process\n","    master_process = True\n","    seed_offset = 0\n","    ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","# note: float16 data type will automatically use a GradScaler\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","# poor man's data loader\n","data_dir = os.path.join('data', dataset)\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","# attempt to derive vocab_size from the dataset\n","meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = None\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    meta_vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihSrf5PufgFR","executionInfo":{"status":"ok","timestamp":1701825806727,"user_tz":480,"elapsed":4,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"9c69d515-3668-4f9b-aea5-24cc519782ca"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tokens per iteration will be: 327,680\n"]}]},{"cell_type":"code","source":["losses = torch.zeros(eval_iters)\n","for k in range(eval_iters):\n","    X, Y = get_batch('val')\n","    with ctx:\n","        logits, loss = model(X, Y)\n","    losses[k] = loss.item()\n","\n","print(f'Memory Usage: {torch.cuda.max_memory_allocated()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"J1GsGLmur5ID","executionInfo":{"status":"error","timestamp":1701825807778,"user_tz":480,"elapsed":1053,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}},"outputId":"def4652d-c343-4e1b-c6a5-d063f2ca65af"},"execution_count":8,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-b00459afe1c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Fall/CS229S/cs229s-final-project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# if we are given some desired targets also calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# inference-time mini-optimization: only forward the lm_head on the very last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacty of 14.75 GiB of which 76.81 MiB is free. Process 5664 has 14.67 GiB memory in use. Of the allocated memory 12.15 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["## Speculative Decoding"],"metadata":{"id":"8MtAm9qQfeOv"}},{"cell_type":"code","source":[],"metadata":{"id":"JJsM06QMfgn2","executionInfo":{"status":"aborted","timestamp":1701825807779,"user_tz":480,"elapsed":4,"user":{"displayName":"John N Wang","userId":"15015754699189480509"}}},"execution_count":null,"outputs":[]}]}